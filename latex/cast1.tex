\documentclass[12pt]{book}
\usepackage[utf 8]{inputenc}
\usepackage[english]{babel}
%\usepackage[IL2]{fontenc}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{color}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage{dsfont}

\usepackage[toc,page]{appendix}

\usepackage{geometry}
 \geometry{
 a4paper,
 %	total={170mm,257mm},
 left = 3cm,
 top = 2.5cm,
 right = 3cm,
 bottom = 2.5cm
 }

\author{Matúš Stehlík}

\setlength{\parindent}{0em}
\setlength{\parskip}{.5em}
\renewcommand\baselinestretch{1.05} % riadkovanie jeden a pol


\newtheorem{thm}[equation]{Theorem}
\newtheorem{lema}[equation]{Lemma}
\newtheorem{cor}[equation]{Corollary}
\newtheorem{prop}[equation]{Proposition}
\newtheorem{hyp}[equation]{Hypothesis}
\theoremstyle{definition}
\newtheorem{defi}[equation]{Definition}
%\newtheorem*{proof}{Proof}
\newtheorem{rem}[equation]{Remark}
\newtheorem{ex}[equation]{Example}


\begin{document}

\section*{Notation}

$\mathbb{R}$ - Set of real numbers \\
$\mathbb{R}_+$ - Set of non-negative real numbers \\
$\mathbb{R}^n$ - Set of $n$-dimensional real vectors   \\
$\mathbb{R}^n_{+}$ -  Set of $n$-dimensional real vectors with non-negative entries  \\

$S^n$ - Unit sphere in $\mathbb{R}^n$ \\


$\mathbb{S}^n$ - Set of symmetric $n\times n$ matrices \\
$\mathbb{S}^n_+$ - Set of symmetric positive semidefinite $n\times n$ matrices \\
$\mathbb{S}^n_{++}$ - Set of symmetric positive definite $n\times n$ matrices \\

$M\succ 0$ - M is symmetric and positive definite \\
$M\succeq 0$ - M is symmetric and positive semidefinite \\
$C\bullet X$ - Matrix inner product $=Tr(C^TX)$\\

PSD - Positive Semidefinite \\

QCQP - Quadratically Constrained Quadratic Program \\
SDP - Semidefinite program \\
SOCP - Second Order Cone Program \\
LP - Linear Program


\chapter{Introduction}

In this thesis we will study quadratically constrained quadratic programs (QCQP)
\defi 
\label{defQCQP}
The Quadratically Constrained Quadratic Program (QCQP) in the standard form is
\begin{equation}
\label{qcqp} 
\begin{array}{ll}
\mbox{minimize}& x^TP_0x + q_0^Tx +r_0 \\
\mbox{subject to}& x^TP_kx + q_k^Tx + r_k \leq 0, \  (k = 1,\dots ,m),
\end{array} 
\end{equation}
where $x\in \mathbb{R}^n$ is a variable, and symmetric $n\times n$ matrices $P_0, P_1, \dots ,P_m \in S^n$, vectors $q_0,\dots ,q_m \in \mathbb{R}^n$ and scalars $r_1,\dots ,r_m\in \mathbb{R}$ are given.

The matrices $P_0, P_1, \dots P_m$ are not necessarily positive semidefinite. Therefore the objective function as well as the constraints may be nonconvex. Problem (\ref{qcqp}) has been proved to be NP-hard in general [\ref{PardalosQPisNPHard}], while several special subclasses
of QCQP have been identified to be polynomially solvable (see [\ref{KimKojimaExactSolViaSDPandSOCP}]).

For example, the 0-1 constraint $x_i\in \{0,1\}$ can be reformulated as $x^Te_ie_i^Tx - e_i^Tx =0$. Thus QCQP includes 0-1 programming, which describes various NP-hard problems, such as knapsack, stable set, max cut etc. 
%Similarly. $\{-1,1\}$ constrained programming, which contains graph partitioning problems.
On the other hand, the above examples suggest that QCQP has many applications and is worth solving.

One of the possible approaches is relaxing QCQP to obtain convex problems which can be solved in polynomial time, namely linear programming (LP), second order cone programming  (SOCP), or semidefinite programming (SDP). 
%Let us define these two classes and see why are they easier to solve.








\chapter{Conic optimization classes}
\label{ConicOptClasses}

In this section we will introduce basic optimization classes mentioned above, in particular the  linear programming (LP), second order cone programming  (SOCP), or semidefinite programming (SDP). We will state the problems in standard forms and their duals. The dual problems will be only mentioned here and will be derived in the next section. We will also show that LP is subclass of convex QCQP, convex QCQP is subclass of SOCP, and SOCP is subclass of SDP, i.e.
\begin{equation}
LP\subset \mbox{convex } QCQP \subset SOCP \subset SDP \subset QCQP .
\end{equation}












\section{Linear programming}

When both, the objective and the constraint functions are linear (affine), the problem is called a linear program and it belongs to the Linear Programming class, or shortly LP.
In this section we will introduce the standard form of LP and its dual. For reference and more information about this topic see i.e. [\ref{BoydCvxOpt}] .

\defi
\label{defLP}
The primal--dual pair of linear programs in standard form is 
\begin{equation}
\label{LP} 
\begin{array}{cc}
Primal & Dual \\
% \hline \\
\begin{array}{ll}
\mbox{minimize} & c^Tx, \\
\mbox{subject to}& Ax = b ,  \\
& x \in \mathbb{R}^n_+,\\

\end{array} 
\ \ \ \ \ & \ \ \ \ \ 
\begin{array}{ll}
\mbox{maximize} & b^Ty, \\
\mbox{subject to}& A^Ty +s = c ,  \\
& s\in \mathbb{R}^n_+, \\
\end{array}
\end{array} 
\end{equation}
where $x\in R^n$, $y\in \mathbb{R}^m$ , $s\in \mathbb{R}^n$ are the variables; the real $m\times n$ matrix $A$ and vectors $b \in \mathbb{R}^m, c\in \mathbb{R}^n$ are given problem data.

\bigskip

\rem Linear programs can be formulated in various forms (including $\geq , \leq$ inequalities, free variables, possibly some linear fractions in objective) but all of them can be transformed to the standard form.

\subsection{Relation to previous classes}
Linear programming is a special case of QCQP, when all matrices in quadratic forms are 0.


\section{Second order cone programing}
\label{SectionSOCP}


The second order cone programming (SOCP) is a convex optimization class which can be solved with great efficiency using interior point methods. 
In this section we will introduce the standard form of SOCP and its dual. For reference and more information about this topic see [\ref{BoydCvxOpt}] .


Let us first define second order cone.
\defi[Second order cone]
\label{defSOC} We say $Q_n$ is second order cone of dimension $n$ if 
\begin{equation}
Q_n=\{x\in \mathbb{R}^n \ | \ x = (x_0,\bar{x}) \in \mathbb{R}\times\mathbb{R}^{n-1}, \|\bar{x}\|_2\leq x_0\}.
\end{equation}


\defi[SOCP]
\label{defSOCP}
The primal--dual pair of the Second Order Cone Program (SOCP) in the standard form is
\begin{equation}
\label{socp} 
\begin{array}{cc}
Primal & Dual \\
%\hline \\
\ \ \ \begin{array}{ll}
\mbox{minimize} & c^Tx,\\
\mbox{subject to}& Ax = b,\\
& x \in Q,
\end{array} 
 \ \ \ \ \ & \ \ \ \ \ 
 \begin{array}{ll}
\mbox{maximize} & b^Ty,\\
\mbox{subject to}& A^Ty + s = c,\\
& s \in Q,
\end{array}
\end{array}
\end{equation}
where $x\in \mathbb{R}^n$ , $y\in \mathbb{R}^m$ and $s\in \mathbb{R}^n$ are the variables; and $m\times n$ real matrix $A$, vectors $c\in \mathbb{R}^n$, $b\in \mathbb{R}^{m},$ $c\in \mathbb{R}^n$, and second order cone $Q$ are given problem data.

\rem Second order cone programs can be formulated in various forms (including quadratic objective or several second order cone constraints of the affine functions), but all of them can be transformed to the standard form. 

\rem In general, any program of the form 
\begin{equation}
\begin{array}{cc}
Primal & Dual \\
%\hline \\
\begin{array}{ll}
\mbox{min} &  c^{1T}x^1+\cdots + c^{kT}x^k,\\
\mbox{s.t.}& A^1x^1+ \cdots + A^kx^k = b, \\
& x^i \in Q_{n_i}, \ (i = 1,\dots ,k),
\end{array} 
 & 
 \begin{array}{ll}
\mbox{max} & b^Ty,\\
\mbox{s.t.}& A^{iT}y + s^i = c^i, \\ %\ (i = 1,\dots ,k)
& s^i \in Q_{n_i}, \ (i = 1,\dots ,k),
\end{array}
\end{array}
\label{genSOCP}
\end{equation}
is considered to be SOCP.
The second order cone constraints can be also formulated as, $x = (x^1,\dots ,x^k) \in Q,$ where $Q$ is Cartesian product of second order cones, 
\begin{equation}
Q = Q_{n_1}\times Q_{n_2}\times \cdots \times Q_{n_k},
\end{equation}
 Since, such Q has all important properties of second order cone (see appendix), 
and algorithmic aspects of solving standard SOCP also work for this more general case [\ref{GoldfarbSOCP},\ref{PokornaSOCPDipl}].
In order to keep things simple, we will sometimes consider only the standard form stated in the Definition \ref{defSOCP},
but all the details can be also done for this more general form.


\subsection{Relation to previous classes}
\label{SectionRelBtwSOCPandQCQP}

Second Order Cone Programming includes convex LP as special case.
We will show that SOCP in fact includes convex QP as a subclass. We will demonstrate procedure proposed in [\ref{KimKojimaSOCPofNoncvxQOP}] used to reformulate convex QP as SOCP.

Let us have convex QCQP. In other words, suppose that $n\times n$ matrices $A_k$, $k=0,\dots ,m$ are positive semidefinite. 
\begin{equation}
\begin{array}{ll}
\mbox{minimize}& x^TP_0x + q_0^Tx + r_0, \\
\mbox{subject to}& x^TP_kx + q_k^Tx + r_k \leq 0, \  (k = 1,\dots ,m),
\end{array} 
\end{equation}
First of all, rewrite problem equivalently as
\begin{equation}
\begin{array}{ll}
\mbox{minimize}& t, \\
\mbox{subject to} & x^TP_0x + q_0^Tx + r_0 \leq t,\\
& x^TP_kx + q_k^Tx + r_k \leq 0, \  (k = 1,\dots ,m).
\end{array} 
\end{equation}

To avoid tedious notation, without loss of generality, suppose that considered program already has linear objective function (i.e. $P_0 = 0$).  Also suppose that we have separated all the linear constraints (ones where $P_k = 0$) and arrange them into more compact form $Ax=b$. Even if we did not, the following procedure will still be correct, but will result in more complicated formulation of linear constraints.

Each convex quadratic constraint 
\begin{equation}
\label{QuadConstraint}
x^TPx + q^Tx + r \leq 0
\end{equation} 
can be transformed into the second order cone constraint.  Suppose that $P\neq 0$ and rank $P = h.$ Then there exists $n\times h$ matrix $L$ such that $P=LL^T$. Such $L$ can be computed by Choelsky factorization of $P$. Now rewrite (\ref{QuadConstraint}) as 
\begin{equation}
\label{ConvertToSOCP1}
(L^Tx)^T(L^Tx)\leq -q^Tx - r.
\end{equation}
It can be easily verified that $w\in \mathbb{R}^t$, $\xi \in \mathbb{R}$ and $\eta \in \mathbb{R}$ satisfy
$$w^Tw\leq \xi\eta, \ \ \xi\geq 0\ \mbox{ and } \ \eta\geq 0$$
if and only if they satisfy
$$\left\|\left( \begin{array}{c}
\xi-\eta\\
2w
\end{array}\right)\right\|_2
\leq \xi+\eta . $$
If we take $w=L^Tx, \ \xi = 1$ and $\eta = -q^Tx - r$, then inequality (\ref{ConvertToSOCP1}) is equivalent to the second order cone constraint
\begin{equation}
\label{ConvertedQCintoSOCC}
 \|v\|_2\leq v_0, \ \mbox{  where  } \ \left(\begin{array}{c}
v_0\\
v
\end{array}\right) = 
\left(\begin{array}{c}
1- q^Tx - r\\
1 + q^Tx + r\\
2L^Tx
\end{array}\right)\in \mathbb{R}^{h+2}.
\end{equation}

Now the intersection of all such second order cone constraints can be easily expressed as Cartesian product of second order cones, thus we have obtained problem of SOCP in form \ref{genSOCP}. 


\section{Semidefinite programming}




The semidefinite programming (SDP) is a convex optimization class which can be solved efficiently using interior point methods. In this section we will introduce the standard form of SDP and its dual. For reference and more information about this topic see [\ref{BoydCvxOpt}] .


Firstly, let us introduce notation we will use to simplify the standard form.

\defi 
\label{defBullet}
Let $A,X$ be real $n\times m$ matrices, we will denote their inner product 
$$A\bullet X = Tr(A^TX).$$
Where $Tr(M)$ denotes trace of matrix $M$ i.e. sum of the diagonal elements of $M$.

\defi[SDP]
\label{defSDP}
The primal--dual pair of the Semidefinite Program (SDP) in the standard form is
\begin{equation}
\label{sdp} 
\begin{array}{cc}
Primal & Dual \\
\begin{array}{ll}
\mbox{minimize} & A_0\bullet X,\\
\mbox{subject to}& A_k\bullet X  = b_k, \ \\
 & (\mbox{for } k = 1,\dots ,m), \\
& X \in \mathbb{S}^n_+,
\end{array}
\ \  &  \ \
\begin{array}{ll}
\mbox{maximize} & b^Ty, \\
\mbox{subject to}& \sum_{k=1}^m y_kA_k + S  = A_0 ,\\
& S \in \mathbb{S}^n_+,
\end{array}
\end{array}
\end{equation}

where $X\in \mathbb{S}^n_+$, $y = (y_1,\dots ,y_m)^T\in \mathbb{R}^m$ and $S\in \mathbb{S}^n$ are the variables; and symmetric matrices $A_0 , A_1,\dots , A_m \in \mathbb{S}^n$ and scalars $b_1,\dots, b_m \in \mathbb{R}$ are given.


Surprisingly, the variable in SDP is a symmetric matrix (not a vector). 
In order to be consistent with other classes we will sometimes use a $svec$ operator.  

\defi \label{svec}
We define operator $svec: \mathbb{S}^n \rightarrow \mathbb{R}^{n(n+1)/2}$, such that for any $n\times n$ symmetric matrix $M$
%\begin{equation}
%svec(M) = (M_{11}, \frac{\sqrt{2}}{2}M_{12}, \dots, \frac{\sqrt{2}}{2}M_{1n}, M_{22}, \frac{\sqrt{2}}{2}M_{23}, \dots, \frac{\sqrt{2}}{2}M_{2n},  \dots, M_{nn})^T
%\end{equation}
\begin{equation}
	svec(M) = ( \delta_{11}M_{11},\delta_{12}M_{12},\delta_{22}M_{22},\dots ,\delta_{1n}M_{1n},\dots ,\delta_{nn}M_{nn})^T,
\end{equation}
where
\begin{equation}
	\delta_{ij}= \left\lbrace 
	\begin{array}{ll} 
		1, &\mbox{ if }\ i=j, \\ 
		\sqrt{2}/2, &\mbox{ otherwise.} 
	\end{array} \right.
\end{equation}

Notice that $\delta_{ij}$ are defined cleverly, so that inner product of symmetric matrices is equivalent to the standard inner product of their images
\begin{equation}
A\bullet X= svec(A)^Tsvec(X), 
\end{equation}
for any pair of symmetric matrices $A,X\in \mathbb{S}^n.$  Now we can easily formulate the problems of SDP in terms of standard inner product over the space of real vectors. 
\begin{equation}
\label{svecSDP}
\begin{array}{ll}
\mbox{minimize} & svec(A_0)^T svec(X),\\
\mbox{subject to}& svec(A_k)^T svec(X)  = b_k, \ (k = 1,\dots ,m), \\
& svec(X) \in \mathcal{K}(\mathbb{S}^n_+),
\end{array}
\end{equation}
where $svec(X)$ is variable,  $\mathcal{K}(\mathbb{S}^n_+) = \{svec(U)\in \mathbb{R}^{n(n+1)/2}\ |\ U\in \mathbb{S}^n_+\}$ 
and problem data are from the standard SDP (\ref{sdp}).


\subsection{Relation to previous classes}
The  SDP primal-dual pair looks suspiciously similar to the both LP and SOCP primal-dual pairs. 
The only difference between LP and SOCP is the nonnegative orthant is replaced by second order cone. 
The SDP, in the $svec$-operator form (\ref{svecSDP}), further generalizes the cone constraint with the semidefinite cone.

In fact, SOCP is subclass of SDP. We will show how the standard SOCP can be rewritten as SDP.
First of all, instead of minimizing $c^Tx$ we will minimize $t$ with additional constraint $t\geq c^Tx$. 
%which we will add to other linear constraints as a new row in $A$ and $b$.

The only nontrivial part is to rewrite conic constraint 
\begin{eqnarray}
& &x\in Q_n \ \Leftrightarrow  \ ||\bar{x}||\leq x_1 \ \Leftrightarrow  
\left\lbrace \begin{array}{r}
\bar{x}^T\bar{x}\leq x_1^2\\
0\leq x_1 
\end{array}\right\rbrace \\
 & &\Leftrightarrow \ 
 \left\lbrace \begin{array}{r}
\frac{\bar{x}^T\bar{x}}{x_1}\leq x_1\\
0\leq x_1 
\end{array}\right\rbrace 
 \ \Leftrightarrow  \ 
 \left( \begin{array}{cc}
x_1 & \bar{x}^T\\
\bar{x} & x_1I_{n-1}
\end{array}\right)\succeq 0.
\label{arrowx}
\end{eqnarray}
Where last equivalence is provided by Schur complement lemma (see appendix, Theorem \ref{SchurCompl}).
From here, it can be easy brought to the standard form (see appendix). 

In case of more general standard form of SOCP (\ref{genSOCP}), the $x\in Q$ constraint 
can be transformed similarly. 
\begin{equation}
x = (x^1,\dots, x^k)^T \Leftrightarrow  M = diag(M_1,\dots, M_k) \succeq 0,
\end{equation}
where $M$ is a block diagonal matrix, with blocks $M_i$ of the form (\ref{arrowx}), for $i=1,\dots, k$,
corresponding to the constraints $x^i\in Q_{n_i}$.

%It can be rewritten as linear matrix inequality (LMI)
%\begin{equation}
%x_1I_n + \sum_{k=2}^n x_kE_{1k} \succeq 0
%\end{equation}
%The linear constraint $Ax = b$ can be easily cast to LMI form with
%%\begin{equation}
%% a_k^Tx = b_k \ \Leftrightarrow \ 
%% \left(\begin{array}{cc}
%% a_{k1}&\bar{a}_k^T/2\\
%% \bar{a}_k/2& 0
%% \end{array}\right)\bullet X = b_k,
%%\end{equation} 
%%where $a_k^T = (a_{k1}, \bar{a}_k^T)$ is the $k$-th row of $A$. 
%\begin{equation}
%Ax=b \ \Leftrightarrow \ diag(Ax-b,b-Ax)\succeq 0,
%\end{equation}
%where $diag(v)$ is the diagonal matrix with entries of $v$ on the main diagonal.
%The resulting 2 LMIs can be simply combined into single one as a block diagonal matrix. The resulting SDP is then
%\begin{equation} 
%\begin{array}{l}
%\mbox{maximize \ }  c^Tx \\
%\mbox{s.t. \ }
%% diag(0,Ax-b,b-Ax)+x_1diag(I_n,0,0) + \sum_{k=2}^n x_kdiag(E_{1k},0,0) \succeq 0.
% diag\left(\begin{array}{c} 0 \\ Ax-b \\b-Ax  \end{array}\right) 
% +  x_1diag\left(\begin{array}{c} I_n \\ 0 \\0  \end{array}\right)
% + \sum_{k=2}^n x_kdiag\left(\begin{array}{c} E_{1k} \\0 \\0  \end{array}\right)\succeq 0.
% \end{array}	
%\end{equation}  


\section{Conic programming}

The previous mentioned classes are quite similar. 
With respect to their variable space, all of them have linear objective, linear constraints and cone constraint.

In fact, they are special cases of the so called conic linear programs.




%Now we can define conic program which generalizes all previous classes.
\defi[Conic Programming] 
\label{defConeProg}
The primal--dual pair of the Linear Conic Program in the standard form is  
\begin{equation}
\label{coneProg} 
\begin{array}{cc}
Primal & Dual \\
\ \ \ \begin{array}{ll}
\mbox{minimize} & c^Tx,\\
\mbox{subject to}& Ax = b,\\
& x \in \mathcal{K},
\end{array} 
 \ \ \ \ \ & \ \ \ \ \ 
 \begin{array}{ll}
\mbox{maximize} & b^Ty,\\
\mbox{subject to}& A^Ty + s = c,\\
& s \in \mathcal{K}^*,
\end{array}
\end{array}
\tag{Conic Program}
\end{equation}
where $x\in \mathbb{R}^n$ , $y\in \mathbb{R}^m$ and $s\in \mathbb{R}^n$ are the variables; and $m\times n$ real matrix $A$, vectors $c\in \mathbb{R}^n$, $b\in \mathbb{R}^{m},$ $c\in \mathbb{R}^n$, and the proper cone $\mathcal{K}$ are given problem data. The 
\begin{equation}
{K}^* = \{z\ |\ \forall x\in\mathcal{K}, \ x^Tz\geq 0\},
\end{equation} 
denotes the dual cone of $\mathcal{K}$ (see appendix \ref{Cones}).

Conic programming contains, but is not limited to, any problems combined from LP, SOCP and SDP programs. 
For example 
\begin{equation}
\begin{array}{ll}
\mbox{minimize} & c^Tx,\\
\mbox{subject to}&A^ix^i=b^i, \ (i = 1,\dots ,k),\\
& x=(x^1,\dots ,x^k) \in \mathcal{K} = (\mathcal{K}^1,\dots ,\mathcal{K}^k),
\end{array} 
\end{equation}
where the variable $x=(x^1, \dots, x^k)^T$ is the Cartesian product of the variables $x^i$, constrained by various LP, SOCP or SDP constraints 
$A^ix^i=b$, $x_i\in \mathcal{K}^i$, where each $\mathcal{K}^i$ is either nonegative orthant, second order cone or semidefinite cone.


This is due to the fact, that all cones we have talked about so far are proper cones (nonegative orthant $\mathbb{R}^n_+$, second order cone $Q_n$ and semidefinite cone $\mathbb{S}^n_+$ as a subset of $ \mathbb{R}^{n(n+1)/2}$ ). 
For proper cones it holds that their Cartesian product is again proper cone (see Section \ref{Cones} in appendix).  

\section{Duality}


\subsection{Duality in conic programming}

We will derive the dual forms of LP, SOCP, SDP all at once by deriving Lagrange dual of general conic program
\begin{equation}
\label{startForConicDuality}
\begin{array}{ll}
\mbox{minimize} & c^Tx,\\
\mbox{subject to}& Ax = b,\\
& x \in \mathcal{K}.
\end{array} 
\end{equation}
The Lagrangian of the problem is given by $\mathcal{L}:\mathbb{R}^n\times\mathbb{R}^m\times\mathcal{K}^*\rightarrow \mathbb{R},$
\begin{equation}
\mathcal{L}(x,y,s) = c^Tx + y^T(b-Ax) - s^Tx. %\ \ \ \mbox{where } s\in \mathcal{K}^*
\end{equation}
The last term (notice that $s\in\mathcal{K}^*$) is added to take account of the conic constraint $x\in \mathcal{K}$.  
It is with negative sign in order to have $\mathcal{L}(x,\cdot,\cdot)\leq c^Tx$ for all $x$ feasible in (\ref{startForConicDuality}). 
Indeed, from the very definition of dual cone:
\begin{equation}
\sup_{s\in \mathcal{K}^*} \ -s^Tx = 
\left\lbrace \begin{array}{ll} 
0 & \mbox{ if } x\in\mathcal{K},\\ 
+\infty & \mbox{ otherwise.}
\end{array}\right. 
\end{equation}
Therefore, the Lagrange dual function is 
\begin{eqnarray}
g(y,s) &=& \inf_x \ \mathcal{L}(x,y,s) \\
%&=& \min_x \ c^Tx + y^T(b-Ax) - s^Tx \\
&=& \inf_x \ y^Tb + (c+A^Ty -s)^Tx \\
&=& \left\lbrace \begin{array}{ll} 
b^Ty & \mbox{ if } c-A^Ty - s = 0,\\ 
-\infty & \mbox{ otherwise.}
\end{array}\right. 
\end{eqnarray}

Hence, the dual problem of linear conic programming in the standard form is 
\begin{equation}
 \begin{array}{ll}
\mbox{maximize} & b^Ty\\
\mbox{subject to}& A^Ty + s = c\\
& s \in \mathcal{K}^*
\end{array}
\end{equation}

Since $\mathbb{R}^n_+$, $Q_n$ and $\mathbb{S}^n_+$ are self-dual, by replacing the $\mathcal{K}$ (and $\mathcal{K}^*$) with any of these cones, we get 
the dual of standard LP, SOCP and SDP as given in the Section \ref{ConicOptClasses}.


\rem \label{doubleDualConeProg} For a closed cone $\mathcal{K}$ it holds that $\mathcal{K}^{**} = \mathcal{K}$ (where $\mathcal{K}^{**}$ denotes dual cone of the dual cone). Therefore, repeating the above procedure, one can easily show that dual of this dual problem is the original primal program (\ref{startForConicDuality}).




\subsection{Duality in QCQP}
\label{sectionDuality}

We will derive dual form of standard QCQP
\begin{equation}
\begin{array}{ll}
\mbox{minimize}& x^TP_0x + q_0^Tx + r_0 \\
\mbox{subject to}& x^TP_kx + q_k^Tx + r_k \leq 0, \  (k = 1,\dots ,m)
\end{array} 
\end{equation}

The Lagrangian of the problem is given by $\mathcal{L}: \mathbb{R}^n\times\mathbb{R}^m_+ \rightarrow \mathbb{R},$
\begin{eqnarray}
\mathcal{L}(x,y) &=&  x^TP_0x + q_0^Tx + r_0 + \sum_{k=1}^m y_k(x^TP_kx + q_k^Tx + r_k) \\
 &=& x^TP(y)x + q(y)^Tx + r(y),
\end{eqnarray}
where %$y\geq 0$ and
\begin{equation} 
P(y) = P_0 + \sum_{k = 1}^m y_kP_k, \ \ \  q(y) = q_0 + \sum_{k = 1}^m y_kq_k, \ \ \ r(y) = r_0 + \sum_{k =1}^m y_kr_k. 
\end{equation}

It holds that $\inf_x\mathcal{L}(x,y) > -\infty$ if and only if $P(y)\succeq 0$ and there exists $\hat{x}$ such that  $P(y)\hat{x} + q(y) = 0.$

Thus, the Lagrange dual function is 
\begin{eqnarray}
g(y) &=& \min_x \ \mathcal{L}(x,y) \\
&=& \left\lbrace \begin{array}{ll} 
-\frac{1}{4}q(y)^TP(y)^{\dagger}q(y) + r(y) & \mbox{ if } P(y)\succeq 0,\ \ q(y)\in\mathcal{R}(P(y))\\ 
-\infty & \mbox{ otherwise,}
\end{array}\right. 
\end{eqnarray}
where $P^\dagger$ denotes Moore-Penrose pseudoinverse of $P$ (see appendix).
Finally, dual form of standard QCQP problem is 
\begin{equation}
\label{QCQD} 
\begin{array}{ll}
\mbox{maximize} & -\frac{1}{4}q(y)^TP(y)^{\dagger}q(y) + r(y), \\
\mbox{subject to}&  y \geq 0,\\
& P(y )\succeq 0, \\
& \mathcal{R}(q(y)) \subseteq \mathcal{R}(P(y)),
\end{array} 
\tag{QCQP Dual}
\end{equation}
where $y\in R^m$ is variable; and problem data $P_0,P_1\dots ,P_m$, $q_0,q_1\dots ,q_m$, $r_0,r_1\dots , r_m$ are given from the primal QCQP above.


This dual problem is basically a SDP (in the LMI form). We first rewrite the objective as linear function $t$ with additional constraint.
\begin{equation}
\begin{array}{ll}
\mbox{maximize} & t ,\\
\mbox{subject to}&  t\leq -\frac{1}{4}q(y)^TP(y)^{\dagger}q(y) + r(y), \\
&y \geq 0,\\
& P(y )\succeq 0,\\
& \mathcal{R}(q(y)) \subseteq \mathcal{R}(P(y)).
\end{array} 
\end{equation}

Due to the Schur complement lemma (see appendix, Theorem \ref{SchurCompl}) the above is equivalent to
\begin{equation}
\begin{array}{ll}
\mbox{maximize} & t, \\
\mbox{subject to}& M:=\left(\begin{array}{ll}
r(y)-t & \frac{1}{2}q(y)^T \\
\frac{1}{2}q(y) & P(y)
\end{array}\right)\succeq 0,
 \\
&y \geq 0,\\
\end{array} 
\end{equation}
where the matrix $M$ is easily expanded as 
\begin{equation}
M= M_0 + \sum_{k=1}^m y_kM_k  - tE
\end{equation}
with
\begin{equation}
M_0 = \left(\begin{array}{rr} r_0& \frac{1}{2}q_0^T\\ \frac{1}{2}q_0 & P_0 \end{array}\right) , \ \ 
M_k = \left(\begin{array}{ll} r_k & \frac{1}{2}q_k^T\\ \frac{1}{2}q_k & P_k \end{array}\right), \ \ 
E =  \left(\begin{array}{ll} 1 & 0_n^T\\ 0_n & 0_{n\times n} \end{array}\right). 
\end{equation}

Now the whole process can be repeated. Create Lagrangian 
$\mathcal{L}_d: \mathbb{R}\times\mathbb{R}^m_+\times\mathbb{S}^{n+1}_+\times\mathbb{R}^m_+ \rightarrow \mathbb{R},$
\begin{equation}
\mathcal{L}_d(t,y,Y,u) = t + Y\bullet M_0 + \sum_{k=1}^m y_kY\bullet M_k - tY\bullet E  + u^Ty,
\end{equation}
where
\begin{equation}
Y = \left(\begin{array}{ll}
x_0 & x^T \\
x & X
\end{array}\right)\succeq 0.
\end{equation}

The Lagrange dual function is 
\begin{eqnarray}
g_d(Y,u) &=& \sup_{t,y} \mathcal{L}_d(t,y,Y,u) \\
&=&  \left\lbrace\begin{array}{ll}
Y\bullet M_0, & \mbox{ if } \left[\begin{array}{l} 
		 					 Y\bullet M_k \leq 0, \ k = 1,\dots ,m, \\
		 					 x_0 = 1 \ \mbox{ and } \
			 				 u = 0_m, 
			 				 \end{array}\right. \\
\infty, & \mbox{otherwise.} 
\end{array}\right. 
\end{eqnarray}

It is easy to see that with $x_0 = 1$ and Schur complement lemma we have
\begin{eqnarray}
Y\bullet M_0 &=& P_0\bullet X + q_0^Tx + r_0, \\
Y\bullet M_k \leq 0 \ &\Leftrightarrow& \ P_k\bullet X+ q_k^Tx + r_k \leq 0,\\
Y\succeq 0 \ &\Leftrightarrow& \ X\succeq xx^T.
\end{eqnarray}

Thus we obtain the following SDP as second dual of QCQP

\begin{equation}
\label{qcqpSecondDual} 
\begin{array}{ll}
\mbox{minimize}& P_0\bullet X + q_0^Tx + r_0\\
\mbox{subject to}& P_k\bullet X+ q_k^Tx + r_k \leq 0, \  (k = 1,\dots ,m)\\
& X\succeq xx^T.
\end{array} 
\end{equation}








\chapter{Relaxations}

As mentioned in the beginning, our strategy is to relax the nonconvex QCQPs (\ref{qcqp}) to easier problem. Let us first explore what is relaxation and how can it be useful. 

Relaxation is usually freely understood as an optimization problem which is obtained by relaxing (loosening) some constraints or even by approximating objective function with a different one. The goal is to obtain a problem which is easier to solve, but still carries some kind of information about the original one. For example, solving the relaxation may give an approximation of the original problem solution.

One could say, that relaxation of minimization problem 
\begin{equation}
\begin{array}{ll}
\mbox{minimize} & f(x), \\
\mbox{subject to} & x\in X,
\end{array}
\end{equation}
is another minimization problem  
\begin{equation}
\begin{array}{ll}
\mbox{minimize} & f_R(x), \\
\mbox{subject to} & x\in X_R,
\end{array}
\end{equation}
with properties $X\subseteq X_R$ and $c_R(x)\leq c(x) \forall x\in X$.
It easily follows, that solving the relaxed problem will provide a lower bound on the optimal value of original problem. In some cases we can also extract a feasible solution of the original problem from solution of the relaxation. In that case we obtain an upper bound for the optimal value. 

Moreover, these bounds may not only give us an idea about the optimal value, but also, may provide means to find an optimal solution of the original problem. 

In the following we will explore the well known SDP  relaxation which is basically casting the nonconvex QCQP into the convex SDP class.
Furthermore, we will introduce some of the approaches for either loosening the SDP relaxation (in order to gain more speed) or strengthening these resulting SDP, SOCP, convex QP and LP relaxations (in order to obtain tighter bounds).



\section{SDP relaxation of QCQP}
\label{SectionSDPRelax}

Since Goemans and Williamson proposed a SDP relaxation of the max-cut problem [\ref{GoemansWilliamsonMaxCut}] that could yield a very good approximation
algorithm, lots of work have been focused on solving the nonconvex QP problems using SDP relaxation methods. In this section we will derive the standard SDP relaxation of QCQP. 




Consider QCQP (\ref{qcqp}). Using identity $x^TP_kx = P_k\bullet xx^T$, which follows from the Definiton \ref{defBullet}, it can be rewritten as follows
\begin{equation} 
\label{1stStepToSDPr}
\begin{array}{ll}
\mbox{minimize}& P_0\bullet X + q_0^Tx +r_0, \\
\mbox{subject to}& P_k\bullet X+ q_k^Tx + r_k  \leq 0, \  (k = 1,\dots ,m)\\
& X = xx^T.
\end{array} 
\end{equation}
Notice, that variable $X$ is symmetric $n\times n$ matrix. 
 The problem can be reformulated in the following way:
 \begin{equation} 
\label{2ndStepToSDPr}
\begin{array}{ll}
\mbox{minimize}& M_0\bullet Y, \\
\mbox{subject to}& M_k\bullet Y \leq 0, \  (k = 1,\dots ,m),\\
& X = xx^T,
\end{array} 
\end{equation}
 where , 
\begin{equation}
M_k = \left(
\begin{array}{cc}
\alpha_k & {1\over 2}q_k^T \\
{1\over 2}q_k & P_k
\end{array}\right), 
\ \ \ \ \ \ \
Y =  \left(
\begin{array}{cc}
1 & x^T \\
x & X
\end{array}\right).
\end{equation}
 This problem still has a non-convex constraint $X = xx^T$, which can be relaxed by a convex constraint, as stated in the following lemma.
  
\lema 
\label{relaxRank1Lemma}
Let $x\in \mathbb{R}^n$, an $n\times n$ symmetric matrix $X$, and $n+1\times n+1$ symmetric matrix $Y$ such that 
$$Y =  \left(
\begin{array}{cc}
1 & x^T \\
x & X
\end{array}\right). 
$$  Then
\begin{itemize}
\item[(i)] $X\succeq xx^T$ if and only if $Y\succeq 0$.
\item[(ii)] $X=xx^T$ holds if and only if $Y\succeq 0$ and \rm $\mbox{ rank } Y=1$. 
\end{itemize}
\proof (i) The statement follows from Schur complement lemma for PSD (see appendix, Theorem \ref{SchurCompl} in appendices) 

(ii) $(\Rightarrow)$ If $X=xx^T$, then also $X\succeq xx^T$, thus $Y\succeq 0$ holds by (i). And since $X=xx^T$,
$$Y=\left(
\begin{array}{cc}
1 & x^T\\
x & xx^T
\end{array}\right)
=  \left(\begin{array}{c}
1\\
x
\end{array}\right)
(1,\ x^T).
$$
Hence $\mbox{rank } Y=1$.\\
$(\Leftarrow)$ Let $Y\succeq 0$ and $\mbox{rank } Y=1$. Since $\mbox{rank } Y=1$, each row of $Y$ must be scalar multiple of the first (obviously nonzero) row $(1,\ x^T)$. To match the first column the $(i+1)$-st row of $Y$ must be $x_i(1,\ x^T)$, for $i=1,\dots , n$.
Therefore, $X=xx^T.$
\qed

\rem Notice that we have proven last implication of (ii) without using $Y\succeq 0$. In fact it is redundant. It also holds that $X=xx^T$ $\Leftrightarrow$ $\mbox{ rank } Y=1$. In fact, there are only 2 options for $Y$ of rank 1: $Y = vv^T$ or $Y = -vv^T$. The second option is easily excluded, because $Y_{11}=1>0$. However, this redundant constraint $Y\succeq 0$ $\Leftrightarrow$ $X\succeq xx^T$ will 
let us keep something from the rank 1 constraint after relaxing it. This approach of adding the redundant constraints (also known as valid inequalities) is often usefull for strengthening the relaxation. For more on valid inequalities see [\ref{HandbookSDP}, \ref{InexactSDPandValidIneq}, \ref{onValidIneqforQP}].




Using the Lemma \ref{relaxRank1Lemma} we obtain another equivalent formulation of the original QCQP
\begin{equation} 
\label{3rdStepToSDPr}
\begin{array}{ll}
\mbox{minimize}& M_0\bullet Y \\
\mbox{subject to}& M_k\bullet Y \leq 0, \  (k = 1,\dots ,m)\\
& Y\succeq 0, \ \ \ \mbox{rank }Y = 1.
\end{array} 
\end{equation}

 relaxing the nonconvex rank 1 constrain we get the following SDP relaxation of (\ref{qcqp})
\begin{equation} 
\label{SDPrelax1}
\begin{array}{ll}
\mbox{minimize}& M_0\bullet Y \\
\mbox{subject to}& M_k\bullet Y \leq 0, \  (k = 1,\dots ,m)\\
& Y\succeq 0.
\end{array} 
\end{equation}
Expanding the terms $M_k\bullet Y$ we obtain the standard SDP relaxation
\begin{equation}
\label{SDPrelax2} 
\begin{array}{ll}
\mbox{minimize}& P_0\bullet X + q_0^Tx + r_0\\
\mbox{subject to}& P_k\bullet X+ q_k^Tx + r_k \leq 0, \  (k = 1,\dots ,m)\\
& X\succeq xx^T.
\end{array} 
\end{equation}

\rem Notice that this is exactly the second dual of QCQP (\ref{qcqpSecondDual}).




The above relaxed problem has different variable space $(X,x)\in \mathbb{R}^{n\times n}\times \mathbb{R}^n$ than original problem $x\in \mathbb{R}^n$. 
In other words, the variable space increased from $O(n)$ to $O(n^2)$ variables.


\section{Convex QP and SOCP relaxation}

These relaxations are expected to provide weaker bounds in less time compared to SDP relaxations. In fact, SOCP relaxations are often constructed as further relaxations of SDP relaxations. So, in certain sense, one can see that SOCP relaxations are never tighter than their SDP counterparts. [\ref{BurerKimKojimaFasterWeakerRelax}]


In this section we will introduce some approaches for further loosening of SDP relaxation. There are 2 main reasons why SDP is expensive to solve. The $O(n^2)$ variable space and semidefinite constraint. 


Using the procedure from section \ref{SectionSOCP}, any convex instance of QCQP can be formulated as SOCP. Therefore we can consider any convex quadratic relaxation as SOCP relaxation. 
Specifically, a convex QCQP relaxation may be represented as 
\begin{equation}
\label{CvxQCQP} 
\begin{array}{ll}
\mbox{minimize}& x^TB_0x + b_0^Tx + \beta_0\\
\mbox{subject to}& x^TB_kx + b_k^Tx + \beta_i \leq 0, \  (k = 1,\dots ,l)
\end{array} 
\end{equation}
where all $B_k\succeq 0$ for $k=0,\dots ,l$. We say that (\ref{CvxQCQP}) is SOCP relaxation of QCQP (\ref{qcqp}) if any $x$ feasible in (\ref{qcqp}) is also feasible in (\ref{CvxQCQP}) and $x^TB_0x+b_0^Tx + \beta_0 \leq x^TP_0x+q_0^Tx+r_0$ holds.



\subsection{Reducing the number of variables}
In [\ref{KimKojimaSOCPofNoncvxQOP}] authors introduce SOCP relaxation of QCQP (\ref{qcqp}) in original variable space $x\in \mathbb{R}^n$. First they assume that objective function is linear (otherwise we can add new variable $t\geq x^TP_0x + q_0^T + r_0$ and then minimize $t$). Then each $P_k$ is written as 
$$P_k=P_k^+ -P_k^-, \ \ \mbox{where } \ P_k^+, P_k^- \succeq 0, \ \ k=1,\dots ,m.$$ 
So that each constraint can be expressed as 
$$x^TP_k^+x + q_k^Tx + r_k \leq x^TP_k^-x.$$
Then an auxiliary variable $z_k\in \mathbb{R}$ is introduced to represent $x^TP_k^-x = z_k$, but also immediately relaxed as $x^TP_k^-x \leq z_k$, resulting in covnex system
\begin{equation}
\begin{array}{rl}
x^TP_k^+x + q_k^Tx + r_k &\leq z_k  \\
x^TP_k^-x & \leq z_k.
\end{array} 
\end{equation}
Finally, $z_k$ must be bounded in some fashion, say as $z_k\leq \mu \in \mathbb{R}$, or else the relaxation would be useless. In this way convex QCQP relaxation is constructed and it is simply transformed to SOCP using the procedure from [\ref{KimKojimaSOCPofNoncvxQOP}] described in section \ref{SectionSOCP}.

\rem Technically it is convex quadratic relaxation. However, it is usually solved in SOCP form by (better developed) algorithms for SOCP, hence it is referred to as SOCP relaxation. 

TODO: (convex QP vs SOCP formulation) That was about 15 years ago.  Find out which one is faster in cvx!


\subsection{SOCP relaxation of semidefinite constraint}

Semidefinite constraint $X-xx^T\succeq 0$  in (\ref{SDPrelax2}) is equivalent to $C\bullet(X-xx^T)\geq 0$ for all $C\in S^n_+$. Using this fact, authors of  [\ref{KimKojimaSOCPofNoncvxQOP}] propose SOCP relaxation of the semidefinite constraint $X-xx^T\succeq 0$ by replacing it with multiple constraints of the form 
$$x^TC_ix-C_i\bullet X \leq 0 \ \ \ (i=1,\dots ,l).$$ 
Since $C_i\succeq 0$, these are convex quadratic constraints and using the procedure described earlier (in section \ref{SectionRelBtwSOCPandQCQP}) one can formulate them as second order cone constraints of the form
\begin{equation}
\label{SOCPRelaxOfPSDconstraint}
\left(\begin{array}{c}
v^i_0\\
v^i
\end{array}\right)
= \left(\begin{array}{c}
1+C_i\bullet X\\
1-C_i\bullet X \\
2L_i^Tx
\end{array}\right), \ \ \ ||v^i||\leq v^i_0, \ \ (i  = 1,\dots ,l)
\end{equation}

The cost of solving the resulting SOCP depends very much on the ranks of $C_i \in \mathbb{S}^n_+$, 
the larger their ranks are, the more auxiliary variables we need to 
introduce and the more expensive the cost of solving the resulting SOCP becomes. In an
attempt to keep the amount of computation small, low rank $C_i$ are
reasonable. Authors of [\ref{KimKojimaSOCPofNoncvxQOP}] suggest that among many candidates, the simplest and reasonable choice is
\begin{equation}
x_i^2 - X_{ii} \leq 0.
\end{equation}
They also employ rank-1 convex quadratic inequalities 
\begin{equation}
x^Tu_iu_i^Tx - u_iu_i^T\bullet X \leq 0 \ (i = 1,\dots ,l),
\end{equation}
where $u_i$ are chosen as eigenvectors of the matrices $P_k$ from the quadratic inequality constraints of original QCQP. 	
This choice of $C_i$ is also essential for reducing the number of variables. 

Authors of [\ref{KimKojimaSOCPofNoncvxQOP}] 
propose following relaxation of $k$-th constraint. For simplicity we will omit the indexing by $k$. 
Let \begin{equation}
P = \sum_{j = 1}^{h}\lambda_j u_ju_j^T, \ \ \ \mbox{ with } \ \ \ P^+ =  \sum_{j=1}^p\lambda_j u_ju_j^T.
\end{equation}
%\begin{equation}
% \lambda_1 \geq \dots \geq \lambda_p \geq 0 > \lambda_{p+1} \geq \dots \geq \lambda_h 
%\end{equation}
then we may rewrite the quadratic inequality constraint $x^TPx + q^Tx + r \leq 0$ as 
\begin{equation}
\begin{array}{l}
\label{kthConstraintReformulation}
x^TP^+x + \sum_{j = p + 1}^h \lambda_j z_j + q^Tx + r \leq 0, \\
x^T(u_ju_j^T)x - z_j = 0, \ ( j= p+1, \dots ,h), 
\end{array}
\end{equation}
and relaxing the last $h-p$ inequalities we have a set of convex inequalities 
\begin{equation}
\begin{array}{l}
\label{SOCPrelax2}
x^TP^+x + \sum_{j = p + 1}^h \lambda_j z_j + q^Tx + r \leq 0 \\
x^T(u_ju_j^T)x - z_j \leq 0, \ ( j= p+1, \dots ,h) 
\end{array}
\end{equation}
It is necessary to add the appropriate constraints on the variables $z_j$ to bound them from above, authors 
show that $\sum_{j=p+1}^h z_j \leq \Vert x\Vert^2$ follows from (\ref{kthConstraintReformulation}).

Now consider the following relaxation of the quadratic constraint  $x^TPx + q^Tx + r \leq 0$
together with valid inequalities relaxing the semidefinite constraint
\begin{equation}
\begin{array}{l}
\label{SOCPrelax3}
P\bullet X + q^Tx + r \leq 0,\\
x^TP^+x - P^+\bullet X \leq 0, \\
x^Tu_ju_j^Tx - u_ju_j^T\bullet X \leq 0, \ (j = p+1,\dots ,h).
\end{array}
\end{equation}
Suppose $(x,X) \in \mathbb{R}^n\times \mathbb{S}^n$ satisfy this relaxation, then $x$ and $z_j = u_j^TXu_j$ for $j = p+1,\dots ,h$ satisfy (\ref{SOCPrelax2}). So we may see (\ref{SOCPrelax2}) as further relaxation of (\ref{SOCPrelax3}). 
For further details about this approach see  [\ref{KimKojimaSOCPRelaxOfPSDconstr}, \ref{KimKojimaSOCPofNoncvxQOP}].



\section{Mixed SOCP-SDP relaxation}

In [\ref{BurerKimKojimaFasterWeakerRelax}] authors have introduced compromise, relaxation of the QCQP (\ref{qcqp}) somewhere between SDP and SOCP. We will describe their approach with the specific case they provide as gentle introduction.
We are dealing with (\ref{qcqp})
\begin{equation}
\begin{array}{ll}
\mbox{minimize}& x^TP_0x + q_0^Tx + r_0 \\
\mbox{subject to}& x^TP_kx + q_k^Tx + r_k \leq 0, \  (k = 1,\dots ,m)
\end{array} 
\end{equation}
Let  $\lambda_{min}(P_k)$ denote smallest eigenvalue of $P_k$.
For all $k=0,\dots ,m$ define $\lambda_k=-\lambda_{min}(P_k)$ so that $P_k + I\lambda_k\succeq 0$. Then (\ref{qcqp}) is equivalent to 
\begin{equation}
\begin{array}{ll}
\mbox{minimize}& -\lambda_0 x^Tx +  x^T(P_0+\lambda_0 I)x + q_0^Tx + r_0\\
\mbox{subject to}& -\lambda_kx^Tx +  x^T(P_k+\lambda_kI)x + q_k^Tx + r_k \leq 0, \  (k = 1,\dots ,m)
\end{array} 
\end{equation}
which has following SOCP-SDP relaxation
\begin{equation}
\label{InBetweenSOCPSDP1}
\begin{array}{ll}
\mbox{minimize}& -\lambda_0 Tr(X) +  x^T(P_0+\lambda_0 I)x + q_0^Tx +r_0 \\
\mbox{subject to}& -\lambda_k Tr(X) +  x^T(P_k+\lambda_kI)x + q_k^Tx + r_k \leq 0, \\  
&(k = 1,\dots ,m) \\
& X\succeq xx^T
\end{array} 
\end{equation}
Notice that other than $X\succeq xx^T$, the only variables in $X$ to appear in the program are diagonal elements $X_{jj}$.
Also, one can see that with fixed $x$ the diagonal entries of $X$ can be made arbitrarily large to satisfy all constraints with $\lambda_k>0$, as well as drive objective to $-\inf$ if $\lambda_0>0$. Therefore, in general, $X_{jj}$ should be bounded to form a sensible relaxation. In the paper they suppose that $x_j\in [0,1]$ and use $X_{jj}\leq x_j$ to establish boundedness.

Next proposition from [\ref{GronePSDcompletions}] gives equivalent formulation of $X\succeq xx^T$ constraint, only in terms of $x$ and diagonal entries of $X$.
\prop Given a vector $x$ and scalars $X_{11},\dots ,X_{nn}$, there exists
a symmetric-matrix completion $X\in S^n$ of $X_{11},\dots ,X_{nn}$ satisfying $X \succeq xx^T$ if and only if $X_{jj} \geq x^2_j$ for all $j = 1,\dots ,n.$ \rm [\ref{GronePSDcompletions}]

Thus, in light of this proposition, the problem with additional bounding constraints $X_{jj}\leq x_j$, the problem (\ref{InBetweenSOCPSDP1}) is equivalent to 
\begin{equation}
\label{InBetweenSOCPSDP2}
\begin{array}{ll}
\mbox{minimize}& -y_0 Tr(X) +  x^T(A_0+y_0 I)x + a_0^Tx \\
\mbox{subject to}& -y_k Tr(X) +  x^T(A_k+y_kI)x + a_k^Tx + \alpha_k \leq 0, \\ &(k = 1,\dots ,m) \\
& x_j^2\leq X_{jj} \leq x_j \ \ (j = 1,\dots ,n)
\end{array} 
\end{equation}
Compared to SDP relaxation (\ref{SDPrelax2}), which has $O(n^2)$ variables, problem (\ref{InBetweenSOCPSDP2}) has only $O(n)$ and hence is much faster to solve. On the other hand bound should be generally weaker than the SDP bound.

This approach is further generalized and explored in [\ref{BurerKimKojimaFasterWeakerRelax}]. 
%Splitting $A= -D + (A+D)$ (instead of $-\lambda_kI + (A +\lambda_kI$)) with clever choice of $C$-block diagonal matrix $D$. 



\section{LP relaxation of QCQP}

Semidefinite program relaxations can provide tight bounds, but they can also be expensive to solve by classical interior point methods (because of the $O(n^2)$ variables and semidefinite constraint). 

Many researchers have studied different types of relaxations, for example, ones based on LP or SOCP. 
As we have seen in the first chapter, SDP is the broadest of the introduced classes, therefore the SDP relaxation will be generally stronger than LP or SOCP relaxation. However the advantage of LP may be the speed. 

Simple LP relaxation may be obtained from SDP relaxation (\ref{SDPrelax2}) replacing the semidefinite constraint $X\succeq xx^T$ with symmetry constraint $X = X^T$.
\begin{equation}
\label{LPrelax1} 
\begin{array}{ll}
\mbox{minimize}& P_0\bullet X + q_0^Tx + r_0\\
\mbox{subject to}& P_k\bullet X+ q_k^Tx + r_k \leq 0, \  (k = 1,\dots ,m)\\
& X = X^T.
\end{array} 
\end{equation}

Suppose that the feasible set is bounded and we can formulate box constraints for each variable - either explicitly included (i.e.  $P_k=0$ for some $k$) or implied from the quadratic constraints (for example in combinatorial problems where constraint $x_i\in \{0,1\}^n$ is included as $x_i(x_i-1) = 0$ we may also include $0\leq x_i \leq 1$ box constraint). We will separate all such box constraints in the next formulation. 

\begin{equation}
\label{LPrelax2} 
\begin{array}{ll}
\mbox{minimize}& P_0\bullet X + q_0^Tx + r_0\\
\mbox{subject to}& P_k\bullet X+ q_k^Tx + r_k \leq 0, \  (k = 1,\dots ,m)\\
 				& l_i\leq x_i \leq u_i, \ (i = 1,\dots ,n)\\
& X = X^T.
\end{array} 
\end{equation}

This is also referred to as lift and project LP relaxation. 

\subsection{Reformulation Linearization Technique (RLT)}


The optimal value of (\ref{LPrelax2}) is usually weak lower bound as no constraint links the values of $x$ and $X$ variables.
The main approach to provide these links and strengthen the relaxation is the Reformulation Linearization Technique (RLT) relaxation [\ref{SheraliAdamsRLT1},\ref{AnstreicherSDPvsRLT},\ref{MargotLPRelax}].
It adds linear inequalities to (\ref{LPrelax2}). These inequalities are derived from the variable bounds and 
constraints of the original problem as follows: multiply together two original constraints or bounds and relax 
each product term $x_ix_j$ with the variable $X_{ij}$. Note that this will be a valid inequality, since the original constraint $X=xx^T$ implies $x_ix_j = X_{ij}$. For instance, let $x_i, x_j$ , $i,j\in \{1, 2,\dots ,n\}$ be two variables from
(\ref{LPrelax2}). By taking into account only the four original bounds $x_i - l_i \geq 0,$ $x_i - u_i \leq 0 ,$ $ x_j - l_j \geq 0,$  $ x_j - u_j \leq 0,$ we get the RLT inequalities
\begin{equation}
\begin{array}{lcl}
X_{ij} - u_ix_j - u_jx_i  &\geq & -u_iu_j ,\\
X_{ij} - u_ix_j - l_jx_i  &\leq & -u_il_j ,\\
X_{ij} - l_ix_j - u_jx_i  &\leq & -l_iu_j ,\\
X_{ij} - l_ix_j - l_jx_i   &\geq & -l_il_j .
\end{array}
\end{equation}
 Note that these constraints also hold when $i = j$, in which case the upper bounds are identical. Denote $l = (l_1,\dots ,l_n)^T$ and $u = (u_1,\dots u_n)^T.$
 Using the vector and matrix inequalities (meaning that inequality holds in each coordinate) the resulting RLT relaxation can be written as 
 
 \begin{equation}
\label{RLTrelax1} 
\begin{array}{ll}
\mbox{minimize}&\ \ P_0\bullet X + q_0^Tx + r_0\\
\mbox{subject to}&\ \ P_k\bullet X+ q_k^Tx + r_k \leq 0, \  (k = 1,\dots ,m)\\
		& \begin{array}{lcl}
		 X - lx^T - xl^T & \geq & -ll^T, \\
		 X - ux^T - xu^T & \geq & -uu^T, \\
		 X - lx^T - xu^T & \leq & -lu^T, 
		 \end{array}\\
 		&\ \ l\leq x \leq u, \ \  X = X^T,
\end{array} 
\end{equation}
In fact it is known that the original bound constraints $l\leq x\leq u$ are redundant and could be removed.
If the QCQP contains linear constraints other than simple box constraints (i.e. $P_k=0$ for some $k$) then additional constraints on $X$ can be imposed.


\subsection{PSD cuts}

In the above LP relaxations we have relaxed nonconvex $X = xx^T$ constraint in (\ref{1stStepToSDPr}) with simple symmetry and later added RLT constraints to link the variables $x$ and $X$. In fact we can go further in strengthening the LP relaxation. 

Let us begin with SDP relaxation in the form (\ref{SDPrelax1}), i.e. 
\begin{equation} 
\begin{array}{ll}
\mbox{minimize}& M_0\bullet Y \\
\mbox{subject to}& M_k\bullet Y \leq 0, \  (k = 1,\dots ,m)\\
& Y\succeq 0,
\end{array} 
\end{equation}
where 
\begin{equation}
M_k = \left(
\begin{array}{cc}
\alpha_k & {1\over 2}q_k^T \\
{1\over 2}q_k & P_k
\end{array}\right), 
\ \ \ \ \ \ \
Y =  \left(
\begin{array}{cc}
1 & x^T \\
x & X
\end{array}\right).
\end{equation}

It is well known, that 
\begin{equation}
Y\succeq 0 \ \Leftrightarrow \ v^TYv \geq 0, \ \forall v\in\mathbb{R}^{n+1}.
\end{equation}
Notice that inequalities $ v^TYv \geq 0$ are linear in $Y$ therefore linear in both $x$ and $X$.
In order to obtain a linear program, the semidefinite constraint cannot be replaced by the infinite number of constraints  $v^TYv \geq 0$.
However, it is possible to relax it with the finite number of these linear inequalities. 

The vectors $v$ may be chosen as eigenvectors corresponding to negative eigenvalues of a arbitrary matrix $\bar{Y}$.
In [\ref{MargotLPRelax}] authors note two weaknesses of such approach. Firstly, only one cut is
obtained from each eigenvector, while computing the spectral decomposition requires non trivial investment of time. Secondly,
such cuts are usually very dense, i.e. almost all entries of $\bar{v}\bar{v}^T$ are nonzero, adding such inequalities might considerably slow down the computation. 

They propose an efficient algorithm to generate sparse cuts from given matrix $\bar{Y}$ and initial vector $\bar{v}$ with $\bar{v}^{T}\bar{Y}\bar{v} \leq 0,$.

TODO: it might be interesting to find out what is the good choice of such $\bar{Y}$ and $\bar{v}$. 
(1 - based on problem data, i.e. matrices from quadratic inequalities, 
2 - iteratively, always take $\bar{Y}$ as optimal solution of previous relaxation and strengthen by cutting it off, until we reach precision of SDP bound)


\section{Strengthening the SDP relaxation}

\subsection{SDP + RLT relaxation}
It is not surprising that adding RLT valid inequalities into LP relaxation will significantly strengthen the relaxation. 
On the other hand, it is quite surprising that RLT inequalities may also help when added to SDP relaxation. 
In fact Anstreicher in \ref{AnstreicherSDPvsRLT} showed that following RLT + SDP relaxation is stronger than both SDP and RLT relaxations.
 \begin{equation}
\label{SDP+RLTrelax1} 
\begin{array}{ll}
\mbox{minimize}&\ \ P_0\bullet X + q_0^Tx + r_0\\
\mbox{subject to}&\ \ P_k\bullet X+ q_k^Tx + r_k \leq 0, \  (k = 1,\dots ,m)\\
		& \begin{array}{lcl}
		 X - lx^T - xl^T & \geq & -ll^T, \\
		 X - ux^T - xu^T & \geq & -uu^T, \\
		 X - lx^T - xu^T & \leq & -lu^T, 
		 \end{array}\\
 		&\ \ l\leq x \leq u, \ \  X \succeq xx^T.
\end{array} 
\end{equation}

However, in terms of computational cost, in his experiments each RLT or SDP bound for the tested problems required approximately 1
second of computation, but each SDP+RLT bound required over 200 seconds of computation. It
is well known that “mixed” SDP/LP problems involving large numbers of inequality constraints
are computationally challenging, and reducing the work to solve such problems is an area of ongoing
algorithmic research. In [\ref{SheraliEnhancingRLT}] they propose enhancing the RLT relaxation with PSD cuts as linear relaxations of the semidefinite constraint. Also adding a SOCP relaxation of semidefinite constraint into lift and project LP relaxation proposed in [\ref{KimKojimaSOCPofNoncvxQOP}] offers a way to avoid the cost of combining SDP and LP constraints.

\subsection{Best D.C. decompositions}
TODO: reconsider completing or deleting this subsection

Another interesting approach for strengthening the SDP relaxation was proposed in [\ref{ZhengDCdecomp}]. 
They restrict to the problems with convex quadratic constraints and nonconvex objective (they suppose 
that $P_k\succeq 0$ for $k = 1\dots m$). 
Let $v_i$ for $i=1,\dots ,p$ be nonzero vectors in $\mathbb{R}^n$ and suppose that there exists $\lambda \in \mathbb{R}^p_+$ and $\mu \in \mathbb{R}^m_+$ such that 
\begin{equation}
P_0 + \sum_{i = 1}^p\lambda_i v_iv_i^T  - \sum_{i = 1}^m \mu_i P_i \succeq 0,
\end{equation}
Then they decompose the nonconvex objective $f_0(x) = x^TP_0x + q_0^Tx + r_0 $ as D.C. (difference of convex functions) 
\begin{equation}
%f_0(x) = 
x^T\left( P_0 + \sum_{i = 1}^p\lambda_i v_iv_i^T  + \sum_{i = 1}^m \mu_i P_i\right)x + q_0^Tx + r_0^T - \sum_{i = 1}^p\lambda_i (v_i^Tx)^2  - \sum_{i = 1}^m \mu_i x^TP_ix.
\end{equation}
After that they convexify the objective by underestimating the nonconvex terms $-\sum_{i = 1}^p\lambda_i (v_i^Tx)^2$ and  $ - \sum_{i = 1}^m \mu_i x^TP_ix$ with a (convex) piecewise linear function, 


and show that optimal choice of parameters $\lambda, \mu$ can be reduced to semidefinite program and show that the resulting SDP has dual equivalent to 
standard SDP (\ref{SDPrelax2}) strengthened with additional valid inequalities.



%\section{Using relaxations to solve the original problem }



\subsection{Branch and bound}



We will first describe a general idea of the branch and bound algorithms.  uppose we are dealing with minimization problem.

The basic idea of this method is to divide and conquer. There are 3 basics steps we will do repeatedly. Branch, compute bounds and prune.
\begin{itemize}
\item Branch. First divide by branching the original problem into the smaller subproblems i.e. by partitioning the feasible set. 
 In combinatorial (discrete) problems this can be done by fixing a variable. For example if $x_l \in \{0,1\}$, then by fixing $x_l=1$ and $x_l=0$ respectively, the problem is divided into two subproblems with one less free variable. 

Although, the branch and bound algorithms are mostly developed and used for solving combinatorial problems, it is also possible to adapt then for general QCQP. Whenever the feasible set is compact, the branching can be done by subdividing the feasible region into the Cartesian product of triangles and rectangles (see [\ref{LinderothSimplicialBranchAndBound}, \ref{RaberSimplicialBranchAndBound}]). 


\item Compute bounds. The lower bounds should be established for each branch. Usually this is done by solving the relaxed problem for each subproblem. 
An upper bound is computed on the optimal value, this can be done by finding a feasible solution for chosen subproblem. It is often extracted from the solution of the relaxed problem using a projection or rounding procedure.

\item Prune. Having both lower and upper bounds, conquer by pruning all the branches with lower bound greater then global upper bound. The optimal solution is surely not in these branches.

\item Repeat. This process is repeated by branching the remaining subproblems, computing the new (better) lower bounds for these smaller problems, improving the upper bound and further pruning the problem tree.
\end{itemize}

One can use more or less clever techniques to decide when, how and which subproblems to branch or how and when to compute bounds.
Using stronger relaxations for bounding steps results in increased computation time, but hopefully less iterations are needed since better bounds allow to cut branches earlier.  On the other hand weaker relaxations are computed faster so they let us do more iterations and explore much greater portion of the problem tree. The greater number of iterations increases chances for guessing the optimal solution as well as obtaining good lower bounds for problems that are small enough.




\chapter{Motivation}


 In this chapter we are going to explore one of the classical problems of combinatorial optimization - the maximum cut problem. First we will state the problem and show its famous SDP relaxation with 0.87856 approximation bound by Goemans and Williamson [\ref{GoemansWilliamsonMaxCut}].

\section{The maximum cut problem}
Let $G = (V, \mathcal{E} )$ be an undirected graph where $V = \{1, \dots , n\}$ and $\mathcal{E}$ are the sets of vertices
and edges, respectively. We assume that a weight $w_{ij}$ is attached to each edge $[i, j] \in \mathcal{E}$.
For a partition $(S, \bar{S})$ of $V$ , we define
\begin{equation}
w(S,\bar{S}) = \sum_{[i,j]\in\mathcal{E}, i\in S, j\in \bar{S}} w_{ij}.
\end{equation}
The maximum cut problem (or shortly max-cut) is to find a partition maximizing $w(S,\bar{S})$.

\subsection{QCQP formulation of the max-cut}
Let us assign zero weight to all the non edges $w_{ij} = 0$ for $[i,j]\notin \mathcal{E}$.
For each $i \in V$, we put
\begin{equation}
x_i = \left\lbrace \begin{array}{rl}
1 & \mbox{if } \ i \in S, \\
-1 & \mbox{if } \ i \in \bar{S}.
\end{array}\right.
\end{equation}

Because $\frac{1}{2}(1-x_ix_j ) = 1$ if $i$ and $j$ belong to different partitions and 0 otherwise, we see that
problem is equivalent to 
\begin{equation}
\label{maxcut2}
\begin{array}{ll}
\mbox{maximize} & \frac{1}{2}\sum_{i<j}w_{ij}(1-x_ix_j ), \\
\mbox{subject to} &  x \in \{-1,1\}^n.
\end{array}
\end{equation}

This can be easily rewritten in matrix form. Let $W$ be a $n\times n$ symmetric matrix of weights with entries $W_{ij} = w_{ij}$, and let
\begin{equation}
\label{LaplacianMatrix}
L = diag(We) - W,
\end{equation}
 where $e$ is all ones vector and $diag(We)$ is the diagonal matrix with diagonal $We$.
Then, using $x_i^2 = 1$ we have
\begin{eqnarray}
w(S,\bar{S}) & = & \frac{1}{2}\sum_{i<j}w_{ij}(1-x_ix_j )  \\
			 & = & \frac{1}{4}\sum_{[i,j]\in V^2}w_{ij}(1-x_ix_j ) \\
			 & = & \frac{1}{4}x^T(diag(We) - W)x  \\
			 & = & \frac{1}{4}x^TLx.
\end{eqnarray}
%\begin{equation}
%\begin{array}{lll}
%w(S,\bar{S}) & = \frac{1}{2}\sum_{i<j}w_{ij}(1-x_ix_j )
%			 & =  \frac{1}{4}\sum_{[i,j]\in V^2}w_{ij}(1-x_ix_j ) \\
%			 & = \frac{1}{4}x^T(diag(We) - W)x 
%			 & =  \frac{1}{4}x^TLx.
%\end{array}
%\end{equation}
%\begin{equation}
%w(S,\bar{S}) =  \frac{1}{4}x^T(diag(We) - W)x = \frac{1}{4}x^TLx.
%\end{equation}
Now, with $x_i \in \{0,1\}$ $\Leftrightarrow x_i^2 = x^Te_ie_i^Tx = 1$ constraint, we obtain a following problem
\begin{equation}
\label{maxcut3}
\begin{array}{ll}
\mbox{maximize} & \frac{1}{4}x^TLx, \\
\mbox{subject to} &  x^Te_ie_i^Tx = 1, \  (i = 1,\dots ,n).
\end{array}
\end{equation}
Therefore max-cut is indeed an instance of QCQP.


\subsection{The Goemans and Williamson analysis}

In their famous article [\ref{GoemansWilliamsonMaxCut}] authors first relax the problem (\ref{maxcut2})
by allowing $x_i$ to be represented by vector variables on the unit sphere $v_i\in S^n$ for $i\in V$. 
And define objective function by replacing the $x_ix_j$ with scalar products $v_i^Tv_j$.
In this way, the objective reduces to $ \frac{1}{2}\sum_{i<j}w_{ij}(1-x_ix_j )$ when all the $v_i$ are lying in 1-dimensional space.
The resulting relaxation is 
 

\begin{equation}
\label{maxcutRelax1}
\begin{array}{ll}
\mbox{maximize} & \frac{1}{2}\sum_{i<j}w_{ij}(1-v_i^Tv_j ), \\
\mbox{subject to} &  v_i \in S^n \ (i = 1,\dots ,n).
\end{array}
\end{equation}
We will show later that this relaxation can be solved using the semidefinite programming.

They propose simple randomized algorithm, which we will refer to as GW-algorithm
\begin{enumerate}
\item Solve (\ref{maxcutRelax1}), obtaining an optimal set of vectors $v_i$.
\item Generate $r$ uniformly distributed on the unit sphere $S^n$.
\item Set $S = \{i\ | \ v_i^Tr \geq 0\}.$
\end{enumerate}
In other words in step 2, the random hyperplane through the origin is chosen. In step 3 we form a partitions based on the separation of the vectors $v_i$ by the hyperplane. The sets $S$ and $\bar{S}$ are formed by indices of vectors lying "above" and "bellow" the hyperplane, respectively.

\prop 
\label{maxcutProp}
Let $\omega$ be the value of the cut produced by the above algorithm and $E[\omega]$ its expected value. Authors show that
\begin{equation}
\label{maxcutExp1}
E[\omega] = 	\frac{1}{\pi}\sum_{i<j}w_{ij}\arccos(v_i^Tv_j ), 
\end{equation}
and 
\begin{equation}
\label{maxcutExp2}
E[\omega]  \geq \alpha\frac{1}{2}\sum_{i<j}w_{ij}(1-v_i^Tv_j) \ \ \mbox{ for } \ \alpha = 0.87856.
\end{equation}

\begin{proof}
First we will show (\ref{maxcutExp1}). 
By definition and linearity of expected value in the first step, using symmetry in the second we have
\begin{eqnarray}
E[\omega] &=& \sum_{i<j} w_{ij} \ Pr\left[sgn(v_i^Tr) \neq sgn(v_j^Tr)\right]  \\
		  &=& \sum_{i<j} w_{ij} \ 2Pr\left[v_i^Tr\geq 0 \ \wedge \ v_j^Tr < 0\right].
		  \label{maxcutlemma1}
\end{eqnarray}
Fix the $i,j$ and let $\theta = \arccos(v_i^Tv_j)$ be the angle between $v_i$ and $v_j$.
In the last expression, there is a probability that $v_i$ is above the hyperplane with normal vector $r$ and $v_j$ is bellow. 
The set $\{r\ | \ v_i^Tr\geq 0 \ \wedge \ v_j^Tr < 0\}$ corresponds to the intersection of two half-spaces whose
dihedral angle is precisely $\theta$ its intersection with the sphere is a spherical
digon of angle $\theta$ and, by symmetry of the sphere, thus has measure equal to
$\theta/2\pi$ times the measure of the full sphere. In other words, 
\begin{equation}
Pr\left[v_i^Tr\geq 0 \ \wedge \ v_j^Tr < 0\right] = \frac{\theta}{2\pi} = \frac{\arccos(v_i^Tv_j)}{2\pi}.
\end{equation}
 Plugging this identity into (\ref{maxcutlemma1}), the first part follows.
 
 Secondly we will show (\ref{maxcutExp2}). It is enough to prove that  
  \begin{equation}
  \label{maxcutlemma2}
\frac{1}{\pi}\arccos(v_i^Tv_j )  \geq \alpha\frac{1}{2}(1-v_i^Tv_j),
 \end{equation}
because multiplying by $w_{ij}$ and summing up these inequalities yelds  
 \begin{equation}
 E[\omega] = 	\frac{1}{\pi}\sum_{i<j}w_{ij}\arccos(v_i^Tv_j )  \geq \alpha\frac{1}{2}\sum_{i<j}w_{ij}(1-v_i^Tv_j),
 \end{equation}
 where the left hand side is equal to $E[\omega]$ from (\ref{maxcutExp1}).
 
 Since $v_i,v_j\in S^n$ are unit vectors, the inner product is bounded by $v_i^Tv_j\in [-1,1]$. Thus, there exists $\theta\in[0,\pi]$ such that $v_i^Tv_j = \cos(\theta)$. Rearranging terms and substituting  $v_i^Tv_j = \cos(\theta)$ we can see that (\ref{maxcutlemma2}) reduces to 
 \begin{equation}
 0.87856 = \alpha \leq \min_{\theta\in[0,\pi]} \frac{2}{\pi}\frac{\theta}{1-\cos(\theta)}.
 \end{equation}
 Which is just a simple exercise to prove.
 
\end{proof}



\rem Denote the objective values 
\begin{eqnarray*}
\omega_r &-& \mbox{value in the optimal solution of the relaxation (\ref{maxcutRelax1})} \\
\omega^* &-& \mbox{value in the optimal solution of the maxcut problem (\ref{maxcut2})} \\
\omega \  &-& \mbox{value in the feasible solution of (\ref{maxcut2}) obtained by the GW-algorithm} 
\end{eqnarray*}
Then the following inequality holds trivially, % by the definition of the optimal solution and its feasibility in relaxed problem,
\begin{equation}
\omega \leq \omega^*\leq \omega_.
\end{equation}

The proposition \ref{maxcutProp} further implies  
\begin{equation}
0.878 \ \omega_r < E[\omega] \leq \omega^*\leq \omega_r.
\end{equation}
Therefore, the bounds obtained by solving the relaxed problem are quite tight. What is more, the feasible solutions projected by GW-algorithm are also good in expectation. Generally, we need to solve the relaxation (\ref{maxcutRelax1}) only once and then repeat steps 2 and 3 of the algorithm couple of times. 
Keeping only the best solution one will (probably) get the objective at least as good as the expected value.


\subsection{Semidefinite relaxation of the max-cut}

Now the last arising question is how do we solve the relaxed problem (\ref{maxcutRelax1})? 
Let us remind the formulation of the relaxed problem
\begin{equation}
\begin{array}{ll}
\mbox{maximize} & \frac{1}{2}\sum_{i<j}w_{ij}(1-v_i^Tv_j ), \\
\mbox{subject to} &  v_i \in S^n \ (i = 1,\dots ,n),
\end{array}
\end{equation}
where $v_i\in S^n$ are the variables, the scalar weights $w_{ij}\in \mathbb{R}_+$ are given
and the indices $i,j$ are in $\{1,\dots ,n\}$.

Let $B := (v_1,\dots ,v_n)$ be the $n\times n$ matrix with vectors $v_i$ as columns. 
It holds that matrix $X:= BB^T$ has elements $ X_{ij} = v_i^Tv_j $ for each $i,j\in (1,\dots,n)$, and $X$ is positive semidefinite.
Since $v_i\in S^n$ are unit vectors, the diagonal entries of $X$ are ones, $X_{ii} = v_i^Tv_i = 1$.
We can find such $X$ by solving following SDP and extract unit vectors $v_i$ from Choelsky decomposition $X = BB^T$.
\begin{equation}
\begin{array}{ll}
\mbox{maximize} & \frac{1}{4}L\bullet X, \\
\mbox{subject to} & diag(X) = e \\
&	X\succeq 0,
\end{array}
\end{equation} 
where $X\in\mathbb{S}^n_+$ is variable, matrix $L$ is defined in (\ref{LaplacianMatrix}), $diag(X)$ is vector of diagonal entries of $X$, and $e$ is the $n$-dimensional vector of ones.

\rem In fact, we would receive the same semidefinite relaxation by following the general scheme for QCQP described in Section \ref{SectionSDPRelax}.

%Let us introduce new variable $X = xx^T$. The problem is then equivalent to 
%\begin{equation}
%\begin{array}{ll}
%\mbox{maximize} & L\bullet X, \\
%\mbox{subject to} & e_ie_i^T\bullet X = 1, \ (i = 1,\dots ,n), \\
%				&	X\succeq 0, \ \ \mbox{rank }X = 1.
%\end{array}
%\end{equation}
%However, each of the constraints $e_ie_i^T\bullet X = 1$ is equivalent to $X_{ii} = 1$, altogether giving $diag(X) = e$, here $diag(X)$ denotes the diagonal of the matrix $X$.
%What is more, relaxing the nonconvex rank 1 constraint we obtain the SDP relaxation 
%\begin{equation}
%\begin{array}{ll}
%\mbox{maximize} & L\bullet X, \\
%\mbox{subject to} & diag(X) = e \\
%&	X\succeq 0.
%\end{array}
%\end{equation} 

\chapter{Computational comparison of the relaxations}

A great comparison of various LP and SDP branch and bound algorithms for the minimum graph bisection problem was done in [\ref{ComparativeBNBStudyForGP}].


%\newpage
%\appendix
%\chapter{Ch1}
%\section{appendix1}
%\defi blablba
%\thm blablbalb

\begin{appendix}


\chapter{ }
\section{Schur complement}

\thm[Schur complement lemma for PSD] 
\label{SchurCompl}
Let $M$ be a symmetric matrix of the form
$$M = \left(\begin{array}{cc}
A & B\\
B^T & C
\end{array}\right).$$ 
The following conditions are equivalent:
\begin{enumerate}
\item $M\succeq 0$\ \ \ ($M$ is positive semidefinite).
\item $A\succeq 0$, \ \ \ $(I-AA^\dagger)B = 0$,\ \ \ $C - B^TA^\dagger B \succeq 0$.
\item $A\succeq 0$, \ \ \ $\mathcal{R}(B)\subseteq \mathcal{R}(A)$,\ \ \ $C - B^TA^\dagger B \succeq 0$.
\end{enumerate}
(The roles of $A$ and $C$ can be switched.)
Where $A^\dagger$ denotes pseudoinverse of $A$ and $\mathcal{R}(B)$ denotes column range of $B$. \rm [\ref{GallierSchurCompl}, \ref{BoydCvxOpt}]

\proof (will be added later)


\section{Cones}
\label{Cones}

\defi[Cone]
\label{defCone}
Set $\mathcal{K}$ is cone if for all $x\in \mathcal{K}$ and all $\theta\geq 0$, it holds that $\theta x \in \mathcal{K}$.

\defi[Proper cone] 
\label{defProperCone}
The cone $\mathcal{K}\subseteq \mathbb{R}^n$ is a proper cone if it has following properties
\begin{itemize}
\item $\mathcal{K}$ is closed
\item $\mathcal{K}$ is convex (for any $\theta_1,\theta_2\geq 0$ and $x_1,x_2\in\mathcal{K}$ also $\theta_1x_1 + \theta_2x_2 \in\mathcal{K}$)
\item $\mathcal{K}$ has nonempty interior  ($int \mathcal{K} \neq \emptyset$)
\item $\mathcal{K}$ is pointed (does not contain whole line, i.e. if $\pm x\in \mathcal{K}$, then $x=0$).
\end{itemize}

\defi[Dual cone]
\label{defDualCone}
For any cone $\mathcal{K}$ we define its dual cone $\mathcal{K}^*$ as a set
\begin{equation}
\mathcal{K}^* = \{z\ |\ \forall x\in\mathcal{K}, \ x^Tz\geq 0\}.
\end{equation}
If  $\mathcal{K} =  \mathcal{K}^*$ we say cone $\mathcal{K}$ is selfdual.

TODO: 
\begin{itemize}
\item carthesian product of proper/self-dual cones
\item $\mathbb{R}^n_+$, $Q_n$, $ \mathbb{S}^n_+$ are proper, self-dual cones + figures.
\item example of converting SOCP to SDP
\item example of converting QP to SOCP
\item Moore-Penrose pseudoinverse

\end{itemize}


\end{appendix}

\chapter*{References}
\addcontentsline{toc}{chapter}{References}

TODO: Unified refeference (same formats and orderings for source, year, pages, volume, journal names )

\begin{enumerate}
\renewcommand*\labelenumi{[\theenumi]}

%
\item S. Burer, S.Kim, M. Kojima, \it Faster, but Weaker, Relaxations for Quadratically Constrained Quadratic Programs, \rm April, 2013 \\
\url{http://www.is.titech.ac.jp/~kojima/articles/socpsdp.pdf}
\label{BurerKimKojimaFasterWeakerRelax}
%
\item S. Kim, M. Kojima, \it Exact Solutions of Some Nonconvex Quadratic Optimization Problems via SDP and SOCP Relaxations,\rm January 2002.Computational Optimization and Applications Vol.26 (2) 143-154 (2003)\\
\url{http://www.is.titech.ac.jp/research/research-report/B/B-375.ps.gz}
\label{KimKojimaExactSolViaSDPandSOCP}
%
\item S. Kim, M. Kojima, M. Yamashita, \it Second Order Cone Programming Relaxation of a Positive Semidefinite Constraint, \rm July 2002. Optimization Methods and Software Vol.18 (5) 535-541 (2003).\\
\url{http://www.is.titech.ac.jp/~kojima/articles/B-381.pdf}
\label{KimKojimaSOCPRelaxOfPSDconstr}
%
\item S. Kim, M. Kojima, \it Second Order Cone Programming Relaxations of Nonconvex Quadratic Optimization Problems, \rm Optim. Methods Softw., 15(3-4):201-224, 2001 \\
\url{http://www.researchgate.net/publication/2637515_Second_Order_Cone_Programming_Relaxation_of_Nonconvex_Quadratic_Optimization_Problems}
\label{KimKojimaSOCPofNoncvxQOP}
%
\item S. Boyd, L. Vandenberghe, \it Convex Optimization, \rm Cambridge University Press 2004, seventh printing with corrections 2009 \\
\url{http://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf}
\label{BoydCvxOpt}
%
\item R. M. Freund, \it Introduction to Semidefinite Programming (SDP), \rm 2009 \\ \url{http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-251j-introduction-to-mathematical-programming-fall-2009/readings/MIT6_251JF09_SDP.pdf}
\label{FreundIntroSDP}
%
\item F. Alizadeh, D. Goldfarb,  \it Second-order cone programming, \rm Springer, 2002 \\
\label{GoldfarbSOCP}
%
\item K. Pokorná, \it Second order cone programing, (Master's thesis), \rm 2013 \\
\url{http://www.iam.fmph.uniba.sk/studium/efm/diplomovky/2013/pokorna/diplomovka.pdf}
\label{PokornaSOCPDipl}
%
\item M. S. Lobo, L. Vandenberghe, S. Boyd, H. Lebret, \it Applications of second-Order cone programming, \rm Linear Algebra and its Applications 284, 1998, 193-228\\
\url{http://www.seas.ucla.edu/~vandenbe/publications/socp.pdf}
\label{LoboVandApplicationsofSOCP}
%
\item J. Gallier, \it The Schur Complement and Symmetric Positive Semidefinite (and Definite) Matrices, \rm December 10, 2010 \\
\url{http://www.cis.upenn.edu/~jean/schur-comp.pdf}
\label{GallierSchurCompl}
%
\item R. Grone, C. Johnson, E. Sá, H. Wolkowicz, \it Positive definite completions of partial hermitian matrices, \rm 
Linear Algebra Applications, 58:109-124, 1984.
\label{GronePSDcompletions}
%
\item H. Wolkowicz, R. Saigal,L. Vandenberghe, \it Handbook of Semidefinite Programming: Theory, Algorithms, and Applications, \rm
ISBN 978-1-4615-4381-7, second printing 2003
\label{HandbookSDP}
%
\item B. Kocuk, S. S. Dey, X. A. Sun, \it Inexactness of SDP Relaxation and Valid Inequalities for Optimal Power Flow, \rm
IEEE Transactions on Power Systems, 2014
\label{InexactSDPandValidIneq}
%
\item H. Dong, J. Linderoth, \it On Valid Inequalities for Quadratic Programming with Continuous Variables and Binary Indicators \rm
Springer Volume 7801 of the series Lecture Notes in Computer Science pp 169-180,
\label{onValidIneqforQP}
%
\item M. X. Goemans, D. P. Williamson, \it Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming \rm J. ACM, 42(6):1115-1145, 1995
\label{GoemansWilliamsonMaxCut}
%
\item K. M. Anstreicher, \it Semidefinite Programming versus the Reformulation Linearization Technique for Nonconvex Quadratically Constrained Quadratic Programming \rm Optimization Online, May 2007\\
\url{http://www.optimization-online.org/DB HTML/2007/05/1655.html}
\label{AnstreicherSDPvsRLT}
%
\item A. Qualizza , P. Belotti, F. Margot, \it Linear Programming Relaxations of Quadratically Constrained Quadratic Programs \rm 
IMA Volume Series 2010 \\
\url{http://wpweb2.tepper.cmu.edu/fmargot/PDF/sdp.pdf}
\label{MargotLPRelax}
%
\item H. D. Sherali, W. P. Adams, \it A reformulation-linearization technique for solving discrete and continuous nonconvex problems \rm
 Kluwer, Dordrecht 1998
 \label{SheraliAdamsRLT1}
%
\item H.D. Sherali, B.M.P. Fraticelli, \it Enhancing RLT relaxations via a new class of semidefinite
cuts \rm J. Global Optim. 22 (2002), 233-261
\label{SheraliEnhancingRLT}
%
\item X. Zheng, X. Sun, D. Li, \it Nonconvex quadratically constrained quadratic programming: best d.c. decompositions and their sdp representations \rm Journal of Global Optimization, 50:695-712, 2011
\label{ZhengDCdecomp}
%

%%%%%%%%% branch and cut
\item J. Linderoth, \it A simplicial branch-and-bound algorithm for solving quadratically constrained quadratic programs \rm 
Mathematical Programming, 103, 251–282 (2005)
\label{LinderothSimplicialBranchAndBound}
%
\item U. Raber, \it A Simplicial Branch-and-Bound Method for Solving Nonconvex All-Quadratic Programs \rm
Journal of Global Optimization 13: 417–432, 1998
\label{RaberSimplicialBranchAndBound}
%
\item M. Armbruster, M. Fügenschuh, Ch. Helmberg, A. Martin, \it LP and SDP branch-and-cut algorithms for the minimum graph bisection problem: a computational comparison \rm
Math. Prog. Comp. (2012) 4:275–306
\label{ComparativeBNBStudyForGP}


%%%%%%%%% miscellaneous
\item P.M. Pardalos, S.A. Vavasis, \it Quadratic programming with one negative eigenvalue is NP-hard \rm J. Global Optim. 1, 15–22, 1991
\label{PardalosQPisNPHard}

\end{enumerate}


\end{document}





In this section we will first talk generally about convex programming, introduce primal and dual probelm and their relationship. In the following subsections we will introduce some important classes of convex optimization problems. Namely linear programming (LP), convex quadratic programming (QP), second order cone programming (SOCP) and finaly semidefinite programming (SDP). 
We will formulate standard forms of these problems and their duals, explain relationships between mentioned classes and briefly state some important properties. For reference and more information about this topic please see [TODO REFS].


Let us first define problem of convex optimization and its lagrange dual.
\defi[Convex Program] We say minimization problem is Convex Program (or shortly just CP) if it has the form of
\label{defCP}
\begin{equation}
\label{CP}
\begin{array}{ll}
\mbox{\rm minimize} & f_0(x) \\
\mbox{\rm subject to}& Ax = b ,  \\
& f_k(x)\leq 0 , \ \ \ k = 1,\dots, m  \\
\end{array} 
\tag{CP}
\end{equation}
Where $x\in \mathbb{R}^n $ is a variable, convex functions $f_0,f_1,\dots ,f_m$ from $\mathbb{R}^n$ to  \\$\mathbb{R},\mathbb{R}^{n_1},\dots ,\mathbb{R}^{n_m}$ respectively, $l\times n$ matrix $A$, and vector $b\in \mathbb{R}^{l}$ are given problem data.

The Dual form of standard Convex Programming problem (or shortly CD) is 
\begin{equation}
\label{CD}
\begin{array}{ll}
\mbox{maximize} & g(y,\nu) \\
\mbox{subject to}& y \in \mathbb{R}^m_+
\end{array} 
\tag{CD}
\end{equation}
Where $y \in \mathbb{R}^m , \nu \in \mathbb{R}^l$ are variables, and function $g: \mathbb{R}^{m+l}\rightarrow \mathbb{R}$ is lagrange dual function 
\begin{equation}
g(y,\nu) = \min_x f_0(x) + \sum_{k=1}^m y_k f_k(x) + \nu^T(Ax-b)
\end{equation}

Following theorem captures relation between these two problems.
{\thm \label{StrongDualityCP}
Let $f_0, f_1,\dots f_m$, $A,b$ be given as above defining (\ref{CP}) and (\ref{CD}). Then 
\begin{enumerate}
\item[\rm (a)] (weak duality)  For any feasible $\bar{x}$ in (\ref{CP}) and any feasible $(\bar{y}, \bar{\nu})$ in (\ref{CD}), it holds that $f_0(bar{x})\geq g(\bar{y}, \bar{\nu})$  Moreover, if $f_0(bar{x}) = g(\bar{y}, \bar{\nu})$  then $\bar(x), (\bar{y}, \bar{\nu})$ are optimal soulutions to (\ref{CP}), (\ref{CD}) respectively.
\item[\rm (b)] (strong duality) If (\ref{CP}) has a strictly feasible solution (Slater's condition holds) i.e. there exists $x_0\in \mathbb{R}$ such that $Ax_0 = b$ and $f_k(x_0)<0$ for all $k = 1,\dots ,m$, then the strong duality holds, that is both problems have optimal solutions, and their values coincide.
\end{enumerate}
}


The dual form of standard Linear Programming problem (or shortly LD) is 
\begin{equation}
\label{LD} 
\begin{array}{ll}
\mbox{maximize} & b^Ty \\
\mbox{subject to}& A^Ty +s = c ,  \\
& s\in \mathbb{R}^n_+
\end{array} 
\tag{LD}
\end{equation}
Where $y\in \mathbb{R}^m$ , $s\in \mathbb{R}^n$ are variables; and problem data $A,b,c$ are given from the primal LP above.
\bigskip


Various properties hold for this (LP)-(LD) pair of problems, let us mention the most important ones.

{\thm
\label{StrongDualityLP}
 Let $A,b,c$ be given as above, defining (\ref{LP}). Then 
\begin{enumerate}
\item[\rm (a)] (weak duality)  For any feasible $\bar{x}$ in primal and any feasible $\bar{y}$ in dual, it holds that $c^T\bar{x}\geq \bar{y}^Tb.$ Moreover, if $c^T\bar{x} = \bar{y}^Tb,$ then $\bar(x), \bar(y)$ are optimal soulutions to primal, dual respectively.
\item[\rm (b)] (strong duality I) If primal has an optimal solution, then so does dual and their values coincide.
\item[\rm (c)] (strong duality II) IF primal and dual both have feasible solutions, then they both have optimal solutions and their optimum values are the same.
\end{enumerate}}

Since this is well known fact, we omit the proof. Notice that part (c) states, that  there is zero duality gap in linear programming with no condition on feasible solutions (only their existence is required). 

%Linear programming is the most basic class of convex optimization. There are fast and effective algorithms for solving LPs [refs].


\subsection{Convex Quadratic Programming}

Linear programming can be easily generalized to by introducing convex quadratic objective function and additional convex quadratic constrains into Quadratic Program (QP) and Quadratically Constrained Quadratic Program (QCQP) respectively.
In this section we will introduce standard and dual form of QP and QCQP, and few important properties. For reference and more information about this topic see [\ref{BoydCvxOpt},   ] 


\defi
\label{defQP}
We say minimization problem is convex Linearly Constrained Quadratic Program (or shortly convex LCQP) if it has form of 
\begin{equation}
\label{LCQP} 
\begin{array}{ll}
\mbox{minimize} & x^TPx + c^Tx \\
\mbox{subject to}& Ax = b ,  \\
& x \in \mathbb{R}^n_+
\end{array} 
\tag{LCQP}
\end{equation}
Where $x\in R^n$ is variable, real $n\times n$ symmetric semidefinite matrix $P\in \mathbb{S}^n_+$, real $m\times n$ matrix $A$ and vectors $b \in \mathbb{R}^m, c\in \mathbb{R}^n$ are given.


\defi
\label{defCQP}
We say minimization problem is convex Quadratically Constrained Quadratic Program (or shortly convex QCQP) if it has form
\begin{equation}
\label{QCQP} 
\begin{array}{ll}
\mbox{minimize} &x^TP_0x + q_0^Tx + r_0 \\
\mbox{subject to}& x^TP_kx + q_k^Tx + r_k \leq 0 , \ \ k = 1,\dots ,m  
\end{array} 
\tag{QCQP}
\end{equation}
Where $x\in R^n$ is variable, $n\times n$ symmetric semidefinite matrices $P_0,P_1,\dots ,P_m\in \mathbb{S}_+^n$ and vectors $q_0,q_1,\dots,q_m \in \mathbb{R}^n$ and scalars $r_0,r_1,\dots ,r_k\in \mathbb{R}$ are given.

\bigskip

The dual form of standard convex quadratic programming problem (or shortly QCQD) is 
\begin{equation}
\label{QCQD} 
\begin{array}{ll}
\mbox{maximize} & -q(y)^TP(y)^{\dagger}q(y) + r(y) \\
\mbox{subject to}&  y \geq 0\\
& P(y )\succeq 0
\end{array} 
\tag{QCQD}
\end{equation}
Where $P^\dagger$ denotes pseudoinverse of $P$, $$ P(y) = P_0 + \sum_{k = 1}^m y_kP_k, \ \ \ \  q(y) = q_0 + \sum_{k = 1}^m y_kq_k, \ \ \ \ r(y) = r_0 + \sum_{k =1}^m y_kr_k. $$
Where $y\in R^m$ is variable; and problem data $P_0,P_1\dots ,P_m$, $q_0,q_1\dots ,q_m$, $r_0,r_1\dots , r_m$ are given from the primal QCQP above.
\bigskip

\subsubsection{Relation to previous classes}

Convex quadratic programs include LP as a special case, by taking matrices $P_0,P_1,\dots ,P_m = 0$.
Also it is easy to see that LP is a proper subclass of convex QCQP, since it does not include quadratic constraints. 
The more solid proof of that is that strong duality from Theorem \ref{StrongDualityLP} does not hold for all convex quadratic programs. 

\ex  Consider following convex QP with variable $x\in \mathbb{R}^2$ and its dual with variable $y\in\mathbb{R}^2$
\begin{eqnarray}
&\mbox{(P)}&\begin{array}{ll}
\mbox{minimize} & x^Tx \\
\mbox{subject to}&  x^Tx + (1,0)x \leq 0\\
& (1,0)x+1\leq 0
\end{array} \\
&\mbox{(D)}&\begin{array}{ll}
\mbox{maximize} & -(y_1 + y_2,0)
diag(\frac{1}{y_1 + 1}, \frac{1}{y_1 + 1}) (y_1 + y_2,0)^T + y_2 \\
\mbox{subject to}&  y \geq 0\\
\end{array} 
\end{eqnarray}
The primal problem ($P$) has only one feasible solution $x = (1,0)^T$ and so its optimal value is 1.
On the other hand solving ($D$) is not that trivial. Nevertheless, after some effort it can be shown that optimal value is $1/4$ for $y = (0,1/2)^T.$

TODO: try to find more trivial example

For convex quadratic programs, and for all the convex programs, there holds  theorem \ref{StrongDualityCP}, which is weaker then strong duality in LP and requires strictly feasible solution. Therefore the Slater's condition, which is not met in the previous example, is necessary. 



% SOCP

\begin{equation}
\label{socd} 
\begin{array}{ll}
\mbox{maximize} & b^Ty\\
\mbox{subject to}& A^Ty+ s = c\\
& s \in Q_n
\end{array} 
\tag{SOCD}
\end{equation}

Where $y\in \mathbb{R}^m$ and $s\in \mathbb{R}^n$ are variables, and problem data $A,b,c,Q_n$ are given from the  primal SOCP above.


TODO: is the following argument and example ok?

On the other hand, not all SOCP problems can be reformulated as convex QP. Therefore convex QP is a proper subclass of SOCP. 
Here is an example of SOCP equivalent to nonconvex QP.
\ex Consider problem (\ref{socp}) from the definition. The conic constraint $x=(x_1,\bar{x})\in Q_n$ can be equivalently formulated as
\begin{equation}
||\bar{x}||\leq  x_1 \ \ \Leftrightarrow \ \ 
\left\lbrace \begin{array}{r}
\bar{x}^T\bar{x}\leq x_1^2\\
0\leq x_1 
\end{array}\right\rbrace
 \ \ \Leftrightarrow  \ \
 \left\lbrace \begin{array}{l}
 x^T
 \left(\begin{array}{cc}
 -1 & 0 \\
 0 &  I_{n-1}
 \end{array}\right)
 x \leq 0 \\
 0\leq x_1
 \end{array}\right\rbrace .
\end{equation}
Where last quadratic constraint is obviously not convex, since the matrix has negative eigenvalue.

%In conclusion, we are able to transform SOCP into QCQP. We can also transform convex QCQP into SOCP. 




% SDP




We say minimization problem is Semidefinite Dual program (or shortly SDD) if it has form
\begin{equation}
\label{sdd} 
\begin{array}{ll}
\mbox{maximize} & b^Ty \\
\mbox{subject to}& \sum_{k=1}^m y_kA_k + S  = A_0 \\
& S \in \mathbb{S}^n_+.
\end{array} \tag{SDD}
\end{equation}


Where $y = (y_1,\dots ,y_m)^T\in \mathbb{R}^m$ and $S\in \mathbb{S}^n$ are the variables. Problem data $A_0,A_1,\dots ,A_m$, $b = (b_1,\dots ,b_m)^T\in \mathbb{R}^m$ are given from the primal problem above.



Alternative formulation of the SDP is the linear matrix inequality form (or shortly LMI)
\begin{equation}
\label{lmi} 
\begin{array}{ll}
\mbox{maximize} & c^Tx \\
\mbox{subject to}& B_0 + \sum_{k=1}^m x_kB_k  \succeq 0. \ 
\end{array} \tag{LMI}
\end{equation}


It is easy to see, that dual problem SDD can be rewritten equivalently in the LMI form.

Any LMI can be brought to standard SDP from by setting $X = B_0 + \sum_{k=1}^m x_kB_k $. The entries of $X$ depend on $x_1,\dots , x_m$ in linear way, which leads to linear relationship between the former, when the latter are eliminated.

On the other hand this process can be reversed. Any SDP can be brought to LMI form by taking $x = (X_{11},X_{12},\dots , X_{nn})$. From $X\succeq 0$ we obtaint $\sum_{i\leq j} E_{ij}X_{ij} \succeq 0$, where $E_{ij}$ is $n\times n$ matrix with all entries 0, but $(i,j)$-th and $(j,i)$-th, equal to 1.
Mentioned LMI will simplify by eliminating entries of $x$ based on linear constraints $A_k\bullet X = b_k$, forming matrices $B_k$ as linear combinations of $E_{ij}$.



\ex[SDP to LMI]
Consider following SDP
\begin{equation}
\begin{array}{ll}
\mbox{minimize} & \left(\begin{array}{rr}1&2\\ 2&3\end{array}\right)\bullet X \\
\mbox{subject to}& \left(\begin{array}{rr}-1&1\\ 1&4\end{array}\right)\bullet X = 5, \\
& X \in \mathbb{S}^2_+.
\end{array} 
\end{equation}
It is equivalent to:
\begin{equation}
\begin{array}{ll}
\mbox{minimize} & y_1 + 4y_2 + 3y_3 \\
\mbox{subject to}&  -y_1 + 2y_2 + 4y_3 = 5,\\ 
& y_1\left(\begin{array}{rr}1&0\\ 0&0\end{array}\right) 
+ y_2\left(\begin{array}{rr}0&1\\ 1&0\end{array}\right)
+ y_3\left(\begin{array}{rr}0&0\\ 0&1\end{array}\right)\succeq 0.
\end{array} 
\end{equation}
Using linear constraint we can eliminate $y_1$, substitute $x_1 = y_2$, $x_2 = y_3$ and obtain 
\begin{equation}
\label{exLMIresult}
\begin{array}{ll}
\mbox{minimize} &  6x_1 + 7x_2 -5 \\
\mbox{subject to}& \left(\begin{array}{rr}-5&0\\ 0&0\end{array}\right) 
+ x_1\left(\begin{array}{rr}2&1\\ 1&0\end{array}\right)
+ x_2\left(\begin{array}{rr}4&0\\ 0&1\end{array}\right)\succeq 0.
\end{array} 
\end{equation}


\ex[LMI to SDP] Consider the above LMI \ref{exLMIresult}. First take $X$ to be a matrix in the semidefinite constraint.
\begin{equation}
\begin{array}{ll}
\mbox{minimize} &   6x_1 + 7x_2 -5 \\
\mbox{subject to}& X = \left(\begin{array}{cc}-5 + 2x_1 + 4x_2 & x_1\\ x_1&x_2\end{array}\right)  \\
& X\succeq 0.
\end{array} 
\end{equation}
The entries of $X$ have the linear relationship  $X_{11} = -5 + 2X_{12} + 4x_{22}.$ We need to express this as linear constraint on $X$. After that we can also reformulate objective as a linear function of $X$.
\begin{equation}
\begin{array}{ll}
\mbox{minimize} &  \left(\begin{array}{cc} 0&3\\ 3&7\end{array}\right)\bullet X  -5 \\
\mbox{subject to}& \left(\begin{array}{rr}-1&1\\ 1&4\end{array}\right)\bullet X = 5  \\
& X\succeq 0.
\end{array} 
\end{equation}
Although this is not exactly the SDP form, it is equivalent to SDP up to constant -5 in objective.




